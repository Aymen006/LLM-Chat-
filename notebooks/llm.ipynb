{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e506c44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env from: /Users/mac/Desktop/LLM Chatbot/.env\n",
      "âœ“ Key loaded successfully and client initialized.\n"
     ]
    }
   ],
   "source": [
    "# !pip install openai\n",
    "# !pip install python-dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = find_dotenv()\n",
    "print(f\"Loading .env from: {dotenv_path}\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Missing OPENAI_API_KEY. Put it in your .env file\")\n",
    "\n",
    "# Strip any quotes that might be included\n",
    "api_key = api_key.strip('\"').strip(\"'\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"âœ“ Key loaded successfully and client initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1585cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-5-nano\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_message_completion(messages, model=\"gpt-5-nano\", temperature=1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def chat_with_context(user_message, context):\n",
    "    \"\"\"\n",
    "    Takes user message and context, gets response from LLM,\n",
    "    and updates context with both user message and assistant response\n",
    "    \"\"\"\n",
    "    # Add user message to context\n",
    "    context.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get response from LLM with full context\n",
    "    assistant_response = get_message_completion(context)\n",
    "    \n",
    "    # Add assistant response to context\n",
    "    context.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d524e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1 equals 2.\n",
      "\n",
      "- In standard base-10 arithmetic, adding 1 and 1 gives 2.\n",
      "- Addition is just combining quantities.\n",
      "- In binary (base-2), 1+1 = 10, which is the binary representation of decimal 2.\n"
     ]
    }
   ],
   "source": [
    "resp = get_completion(\"what is a 1+1 in math\")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b33c175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio Version: 6.3.0\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install --upgrade gradio\n",
    "import gradio as gr\n",
    "import json\n",
    "from db_functions import get_or_create_coach_state, save_coach_state\n",
    "\n",
    "print(f\"Gradio Version: {gr.__version__}\")\n",
    "\n",
    "# System prompt for the coach\n",
    "COACH_SYSTEM_PROMPT = \"\"\"You are a mentor-coach for high-pressure individuals such as founders. For each session, you will receive:\n",
    "- **COACH_STATE:** a JSON containing the user's long-term goals, plans, blockers, preferences, and commitments. Treat this as authoritative.\n",
    "- **RECENT_TURNS:** the user's most recent conversation exchanges.\n",
    "- **The user's latest message.**\n",
    "\n",
    "Your outputs must adhere to these instructions:\n",
    "\n",
    "- Use **COACH_STATE** as the reliable source of the user's objectives, blockers, and commitments.\n",
    "- Use **RECENT_TURNS** for short-term context and recent conversational flow.\n",
    "\n",
    "For each response, follow this structured format:\n",
    "\n",
    "1. **Pattern & Stress Signals**: Begin by analyzing the user's latest message for stress indicators, recurring patterns, and potential mindset pitfalls. Reference COACH_STATE and RECENT_TURNS for accuracy.\n",
    "2. **Recommendations**: Based on the above, provide highly specific, actionable advice that speaks to the user's context, closing the gap between their goals and current situation. Avoid generic or repetitive suggestions.\n",
    "3. **Next Actions (checklist)**: Provide a short, actionable checklist (2â€“4 items max) for what the user should do before the next check-in.\n",
    "4. **Follow-up Questions (if needed)**: Pose 1â€“3 sharp clarifying questions that help flesh out missing details about goals, blockers, preferences, commitments, or timelines. Only ask if this information is not precise in COACH_STATE or RECENT_TURNS.\n",
    "\n",
    "**Rules:**\n",
    "- Do NOT fabricate or infer facts, deadlines, or commitments not present in COACH_STATE or RECENT_TURNS.\n",
    "- If key information is missing, use targeted follow-up questions to uncover it.\n",
    "- If the user reports progress, recognize it and tailor new advice precisely, but do not assert that memory has been updated (this process is external).\n",
    "- Keep language concise, direct, and accountability-focused. Avoid platitudes or soft generalities.\n",
    "\n",
    "**Output Format**: Human-readable, no JSON or code formatting. Use clear, separate section headers for each part (Pattern & Stress Signals, Recommendations, Next Actions, Follow-up Questions).\n",
    "\n",
    "**Reminder:** Always ground your responses in the provided COACH_STATE and RECENT_TURNS. Never fabricate information, commitments, or deadlines not found there. Organize your answer using the mandatory output sections.\n",
    "\"\"\"\n",
    "\n",
    "MEMORY_UPDATER_PROMPT = \"\"\"\n",
    "You are a memory updater for a coaching chatbot.\n",
    "Input:\n",
    "- OLD_COACH_STATE (JSON)\n",
    "- DIALOGUE_CHUNK (recent turns from the CURRENT session, in-memory)\n",
    "\n",
    "Task:\n",
    "Update OLD_COACH_STATE using ONLY facts explicitly stated in DIALOGUE_CHUNK.\n",
    "Return ONLY the UPDATED_COACH_STATE as valid JSON. No extra text.\n",
    "\n",
    "Special requirement:\n",
    "Update \"pattern_analysis\" and \"last_emotional_state\" to reflect how the user presented in this session.\n",
    "- Derive emotional state ONLY from explicit wording/tone in DIALOGUE_CHUNK.\n",
    "- Keep it compact and conservative; do not over-infer.\n",
    "- Use short strings for signals/patterns; avoid long narrative.\n",
    "\n",
    "Rules:\n",
    "- Add or modify goals only if the user clearly stated them.\n",
    "- Add next actions only if the user explicitly committed to them.\n",
    "- Mark actions done ONLY if user confirmed completion.\n",
    "- Track blockers only if clearly described.\n",
    "- Track preferences only if explicitly stated.\n",
    "- Do NOT store secrets (API keys, passwords).\n",
    "- Keep IDs stable if present.\n",
    "\n",
    "Required keys (must always exist):\n",
    "{\n",
    "  \"user_profile\": { \"name\": null, \"preferences\": { \"tone\": \"direct\", \"accountability\": \"high\", \"constraints\": [] } },\n",
    "  \"goals\": [],\n",
    "  \"current_focus\": \"\",\n",
    "  \"next_actions\": [],\n",
    "  \"plan\": [],\n",
    "  \"blockers\": [],\n",
    "  \"open_loops\": [],\n",
    "  \"pattern_analysis\": {\n",
    "    \"overall_tone\": \"neutral\",\n",
    "    \"stress_level\": 0,\n",
    "    \"dominant_emotions\": [],\n",
    "    \"confidence_level\": 0,\n",
    "    \"signals\": [],\n",
    "    \"recurring_patterns\": [],\n",
    "    \"last_session_notes\": \"\"\n",
    "  },\n",
    "  \"last_emotional_state\": {\n",
    "    \"mood_label\": \"neutral\",\n",
    "    \"valence\": 0,\n",
    "    \"arousal\": 0,\n",
    "    \"risk_flags\": []\n",
    "  },\n",
    "  \"last_session_summary\": \"\",\n",
    "  \"updated_at\": \"\"\n",
    "}\n",
    "\n",
    "Return JSON only.\n",
    "\"\"\"\n",
    "\n",
    "def update_coach_state(old_state, dialogue_chunk):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": MEMORY_UPDATER_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"OLD_COACH_STATE: {json.dumps(old_state)}\\n\\nDIALOGUE_CHUNK: {dialogue_chunk}\"}\n",
    "    ]\n",
    "\n",
    "    # Using a model capable of good JSON generation\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=messages,\n",
    "        temperature=1,\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    \n",
    "    new_state_json = response.choices[0].message.content\n",
    "    try:\n",
    "        return json.loads(new_state_json)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON from memory updater\")\n",
    "        return old_state\n",
    "\n",
    "def load_user_state(user_id):\n",
    "    if not user_id or user_id.strip() == \"\":\n",
    "        return None, [], [], \"Please enter a User ID to start.\"\n",
    "    \n",
    "    user_id = user_id.strip()\n",
    "    state = get_or_create_coach_state(user_id)\n",
    "    goals_preview = state.get('goals', [])[:3]\n",
    "    goals_text = f\" Goals: {goals_preview}\" if goals_preview else \"\"\n",
    "    return user_id, [], [], f\"âœ“ Loaded state for user: {user_id}.{goals_text}\"\n",
    "\n",
    "def process_message(user_message, history, user_id, conv_history):\n",
    "    if not user_id:\n",
    "        return history, conv_history, \"âš  Please load a User ID first.\"\n",
    "    if not user_message or user_message.strip() == \"\":\n",
    "        return history, conv_history, \"\"\n",
    "    \n",
    "    coach_state = get_or_create_coach_state(user_id)\n",
    "    messages = [{\"role\": \"system\", \"content\": COACH_SYSTEM_PROMPT}]\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"COACH_STATE:\\n{json.dumps(coach_state, indent=2)}\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": \"I've reviewed the COACH_STATE.\"})\n",
    "    \n",
    "    # clean_conv_history ensures we don't mix up types\n",
    "    clean_conv_history = [m for m in conv_history if isinstance(m, dict) and 'role' in m] if conv_history else []\n",
    "    for msg in clean_conv_history:\n",
    "        messages.append(msg)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    try:\n",
    "        response = get_message_completion(messages)\n",
    "    except Exception as e:\n",
    "        return history, conv_history, f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Append to internal history (dicts for LLM)\n",
    "    new_conv_history = clean_conv_history + [\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    # For Gradio 6.x: Use messages format (dictionaries) for the chatbot display\n",
    "    if history is None: history = []\n",
    "    new_history = history + [\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    return new_history, new_conv_history, \"\"\n",
    "\n",
    "def update_memory(user_id, conv_history):\n",
    "    if not user_id: return \"âš  No user loaded.\"\n",
    "    if not conv_history: return \"âš  No conversation to save.\"\n",
    "    \n",
    "    recent_history = conv_history[-40:]\n",
    "    dialogue_chunk = \"\\n\".join([\n",
    "        f\"{msg['role'].capitalize()}: {msg['content']}\" \n",
    "        for msg in recent_history if isinstance(msg, dict) and 'role' in msg\n",
    "    ])\n",
    "    \n",
    "    old_state = get_or_create_coach_state(user_id)\n",
    "    try:\n",
    "        new_state = update_coach_state(old_state, dialogue_chunk)\n",
    "        required_keys = ['user_profile', 'goals', 'pattern_analysis', 'last_emotional_state']\n",
    "        if not isinstance(new_state, dict) or not all(k in new_state for k in required_keys):\n",
    "            return \"âš  Memory update failed: invalid state.\"\n",
    "        save_coach_state(user_id, new_state)\n",
    "        return f\"âœ“ Memory updated for {user_id}.\"\n",
    "    except Exception as e:\n",
    "        return f\"âš  Error: {str(e)}\"\n",
    "\n",
    "with gr.Blocks(title=\"AI Coach\") as demo:\n",
    "    gr.Markdown(\"# AI Coaching Assistant (v4-Gradio-6.x-Compatible)\")\n",
    "    gr.Markdown(\"Enter your User ID to load your coaching state.\")\n",
    "    \n",
    "    current_user_id = gr.State(value=None)\n",
    "    conversation_history = gr.State(value=[])\n",
    "    \n",
    "    with gr.Row():\n",
    "        user_id_input = gr.Textbox(label=\"User ID\", placeholder=\"Enter your name or ID...\")\n",
    "        load_btn = gr.Button(\"Load State\", variant=\"primary\")\n",
    "    status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
    "    \n",
    "    # Gradio 6.x uses messages format (dict) by default\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=400)\n",
    "        \n",
    "    msg_input = gr.Textbox(label=\"Your message\", placeholder=\"Type your message here...\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "        save_btn = gr.Button(\"ðŸ’¾ Update Memory\", variant=\"secondary\")\n",
    "    gr.Markdown(\"*Click 'Update Memory' to save your session progress.*\")\n",
    "    \n",
    "    load_btn.click(fn=load_user_state, inputs=[user_id_input], outputs=[current_user_id, chatbot, conversation_history, status_text])\n",
    "    send_btn.click(fn=process_message, inputs=[msg_input, chatbot, current_user_id, conversation_history], outputs=[chatbot, conversation_history, msg_input])\n",
    "    msg_input.submit(fn=process_message, inputs=[msg_input, chatbot, current_user_id, conversation_history], outputs=[chatbot, conversation_history, msg_input])\n",
    "    save_btn.click(fn=update_memory, inputs=[current_user_id, conversation_history], outputs=[status_text])\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
